"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10MtTNTSYp5opxMpcN-Gfk4VYW8ZyJ3Qa
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import sys


# Read the KDDcup dataset into a feature matrix
def dataset(train, test):
    print('Loading Dataset...')
    def read(filename):
        # Read the column names
        with open('kddcup.names') as f:
            labels = f.readline()[:-2].split(',')
            names = [line.split(':')[0] for line in f]
            names.append('label')

        # The data values into a dataframe
        data = pd.read_csv(filename, header=None)
        data.columns = names

        return data

    train, test = read(train), read(test)
    data = train.append(test, ignore_index=True)

    # Convert labels to good(0) and bad(1) labels
    data.label = data.label.map({'normal.': 0}).fillna(1)

    # Convert categorical variables to one-hot
    data = pd.get_dummies(data)

    # Seperate the features and labels 
    labels = data.label
    del data['label']

    # standardize data
    #     data = (data-data.min())/(data.max()-data.min())
    #     data = data.fillna(0)

    data, labels = data.values, np.expand_dims(labels.values, 1)

    return data[:len(train)], labels[:len(train)], data[len(train):], labels[len(train):]


# Define all parameters/hyperparameters
EPOCHS = int(sys.argv[1])
BATCH_SIZE = 1
SEQ_LEN = 97
blocks = int(sys.argv[2])
rnn_size = int(sys.argv[3])
rate = 0.001
peephole = True if sys.argv[4] == 'y' else False


# Create a LSTM layer of given size
def make_cell(size, peephole=False):
    return tf.contrib.rnn.LSTMBlockCell(size, reuse=tf.AUTO_REUSE, use_peephole=peephole)


# Returns a computational graph for LSTM architecture
def LSTM(x, y, init_state, blocks, size, peephole=False):
    # Reshape tensor to make seq length as first dimension
    x = tf.unstack(x, SEQ_LEN, 1)
    y = tf.unstack(y, SEQ_LEN, 1)
    #     init_state = cell.zero_state(BATCH_SIZE, tf.float32)

    # Define the output layer weights
    W_out = tf.Variable(tf.truncated_normal(shape=(size, 1), mean=0, stddev=0.1))
    b_out = tf.Variable(tf.zeros(1))

    # Define the multi layer LSTM architechture
    cell = tf.nn.rnn_cell.MultiRNNCell([make_cell(size, peephole) for _ in range(blocks)])
    output, state = tf.nn.static_rnn(cell, x, initial_state=init_state)

    # Define output layer computation + loss
    logits = [tf.matmul(out, W_out) + b_out for out in output]
    losses = [tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=label) for logit, label in zip(logits, y)]
    total_loss = tf.reduce_mean(losses)

    return total_loss, logits, state


# Read and prepare kddcup dataset
x_train, y_train, x_test, y_test = dataset('kddcup.data_10_percent_corrected', 'kddcup.data.corrected')

# Define placeholders for Tf computational graph
x = tf.placeholder(tf.float32, [None, SEQ_LEN, x_train.shape[-1]])

y = tf.placeholder(tf.float32, [None, SEQ_LEN, 1])
y_unstack = tf.unstack(y, SEQ_LEN, 1)

init_state = tf.placeholder(tf.float32, [blocks, 2, BATCH_SIZE, rnn_size])
rnn_state = tf.unstack(init_state, blocks, 0)
rnn_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(rnn_state[i][0], rnn_state[i][1]) for i in range(blocks)])

# Setup optimizer
loss, logits, state = LSTM(x, y, rnn_state, blocks, rnn_size, peephole)
training_op = tf.train.AdamOptimizer(rate).minimize(loss)

# Setup performance metrics
correct_prediction = tf.equal(tf.round(tf.sigmoid(logits)), tf.unstack(y, SEQ_LEN, 1))
accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

predicted, actual = tf.round(tf.sigmoid(logits)), y_unstack
TP = tf.count_nonzero(tf.multiply(predicted, actual))
TN = tf.count_nonzero(tf.multiply(tf.subtract(predicted, 1), tf.subtract(actual, 1)))
FP = tf.count_nonzero(tf.multiply(predicted, tf.subtract(actual, 1)))
FN = tf.count_nonzero(tf.multiply(tf.subtract(predicted, 1), actual))


# Start a new tf session
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    num_examples = x_train.shape[0]

    # Training LSTM on training set using mini-batches
    print("Training...")
    print()
    for i in range(EPOCHS):
        acc_total = 0.0
        loss_total = 0.0
        count = 0
        curr_state = np.zeros((blocks, 2, BATCH_SIZE, rnn_size))
        for offset in range(0, num_examples, SEQ_LEN):
            end = offset + SEQ_LEN
            batch_x, batch_y = np.expand_dims(x_train[offset:end], 0), np.expand_dims(y_train[offset:end], 0)
            _, loss_curr, curr_state, acc = sess.run([training_op, loss, state, accuracy_op], feed_dict={
                x: batch_x,
                y: batch_y,
                init_state: curr_state
            })
            acc_total += acc
            loss_total += loss_curr
            count += 1
        #             if count % 5 == 0:
        #                 print(loss_curr/count, acc_total/count)

        print(f"Epoch {i+1} :", acc_total/count)

    # Testing accuracy after completing training
    print('Testing...')
    print()
    curr_state = np.zeros((blocks, 2, BATCH_SIZE, rnn_size))
    num_examples = x_test.shape[0]
    tp, tn, fp, fn = 0, 0, 0, 0

    for offset in range(0, num_examples - SEQ_LEN, SEQ_LEN):
        end = offset + SEQ_LEN
        batch_x, batch_y = np.expand_dims(x_test[offset:end], 0), np.expand_dims(y_test[offset:end], 0)
        tp_, tn_, fp_, fn_ = sess.run([TP, TN, FP, FN],
                                      feed_dict={
                                          x: batch_x,
                                          y: batch_y,
                                          init_state: curr_state
                                      })
        tp += tp_
        tn += tn_
        fp += fp_
        fn += fn_

    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1 = 2 * precision * recall / (precision + recall)

    print(f"Trained for {EPOCHS} EPOCHS")
    print("Final Test Accuracy = {:.3f}".format(accuracy))
    print("Final Test Recall = {:.3f}".format(recall))
    print("Final Test F1-score = {:.3f}".format(f1))